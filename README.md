# Capital-Asset-Pricing-Model
### Understanding CAPM through linear regression approach, then it is just different format

In linear regression:
y = b + b1(x) + e
- y is dependent value
- b is the y intercept
- b1 is the slope. (from the perspective of graph, from the perspective of formula, it is b1, from practical meaning, it is when the IV change 1 unit, then the DV will change b1 unit)
- x is the independent value, also the predictor
- e is the error terms

In CAPM terms:
return of asseti = alpha + beta(market return) + error term
- alpha is the y intercept, the rturn of the stock itself
- beta is the slope of the regression line, the correlation of the stock to the market return
- market return is the benchmark, can choose disecrete index

Generally, beta info in Yahoo Finance doesnt consider the risk-free rate, if consider, it would be:
return of asseti - rf = aplha + beta(market return-rf) + error term

### Building the regression model
- stats module has no constant as default
- users need to add one on their own
- OLS stands for ordinary least squares, which is an approach to get a min SSE, max SSR, so the regression line has the biggest explanatory power of the dependent variable
- use the seaborn module to plot the regression model

### Statistical Concept Revision and Interpretation of the results
- refer to the notes of the jupyter notebook file, I mentioned highlights here
- R-sqaure is high, and has no big drop after adjusted(adjusted by number of features we used to explain the y variance, and the size of samples we have, the more feaures, or smaller sample size, the smaller adjusted R-square), indicating it has strong explanatory power and prediction power
- pay attention to the 95% confidence interval of b1 coefficient, and the p-value. Why?
- since we are concerning the possibillity of the b1 being 0, which means x,y has no linear relationship, and our regression model is meaningless
- looking at the condidence inerval we have, it doesnt include 0, it is good, but by how much chance it doesnt include 0?
- we look at the t-ratio of it first, we know t-critical value for a two-tailed test, aplha took 5%, is around 2.77, now, we have 23>2.77, which means more than 95% data are covered in reality. Therefore, an alpha much smaller than 5% we are having with this regression formula
- also, p value is 0, or very smaller, much smaller than .05
- these observations make us reject the null hypothesis that b1=0, since now the chance for us to commit type 1 error is much smaller than 5% threshold we set, we are confident to reject the null now
- pay attention to b1 generated by the regression formula, it is just the mean of the estimation of the real b1, or we can say it is the expected value of the real b1. With 95% confidence, we claim the confidence interval contains the true b1

### Remark of matrix multiplication in the residual analysis part:
- the goal is to find [b1,b0]
- Geometrically interpreting the condition is, we have [b1,b0] as vector v at the beginning, it has 2rows1col, it means a single vector in a 2D plane, it's position is denoting by 2 coordinates b1, b0
- then it is transformed by a matrix A, to a new vector p, it has nrows1col, it means a single vector in a nD plane, it's position is denoting by n coordinates y1 ... yn 
- so practically, we have to create matrix A first, while we only have x1 as ^GSPC-rf , so we need to create an array of 1 with the same row number of x1
- then we can find b1,b0 by timing the new vector p with inverse matrix of A. Thats the work done by np.linalg.lstsqr
